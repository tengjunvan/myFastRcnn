写了个vgg16+rpn的网络
一个网络的主要结构有：
1.dataloader（这一部分花费时间蛮多，主要是理解数据，还有就是在vgg16+rpn的数据预处理的时间比较多）
2.optimizer（这里涉及到了weight decay）
3.网络模型model
4.可视化工具visdom(这一部分我倒是没有写)
5.一个warper 叫做trainer整合上面的东西


loss是cls+reg两个loss之和
数据是8000张图片
epoch是一次，花费时间五个小时

这里引出了我的一个问题，固然model或者optimizer都可以很快的确定，网络参数初始化也可以从文章中快速确定，但是例如，epoch数，learning rate，以及weight decay，这些参数是和训练时候的表现可以自动的调整的（比如loss下降的比较慢，调高learning rate等等），这些策略应该怎么定。如何利用val来确定hyperparameter这一点我也没有实现。


2020/1/29：
写一个visdom or tensorboard的可视化模块（未完成）
阅读了超参的选择，涉及了gp和bayesian optimization

2020/1/30

2020/2/8：
在编写non-maximum-supression过程中，我发现他原本的代码很复杂，似乎用gpu计算这个过程，这里涉及到一个问题，就是平日里编写网络中，比如在一个layer中是一个有numpy 计算的，能用model.cuda()来整个仍给gpu来计算吗，真的不用手写cuda代码吗。
